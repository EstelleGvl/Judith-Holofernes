---
title: "Notes 6: Neural Networks"
output: html_document
---

```{r, message=FALSE}
library(readr)
library(dplyr)
library(ggplot2)
library(stringi)
library(jpeg)
library(igraph)

theme_set(theme_minimal())
source("scripts/funs.R")
```

# Neural network models

So far we have seen several ways of constructing features that describe visual data. We could
spend many more days describing increasingly complex ways of extracting interesting features
from our data. This is exactly what the field of computer vision did for decades in order to
capture higher-complexity features from image data. Around 2006, give or a take a bit, there
was an explosion in the interest of a certain type of model called a deep convolutational neural
network (CNN). These are data-oriented methods, that is, they *learn* from tagged data rather
than by the explicit construction by human experts. 

While there have been great strides in making CNNs usable by non-specialists, there are still
two hurdles to their application. First, the models take a while to run over a dataset (unless
you have access to a special kind of hardware called a GPU). Secondly, the software can be a
bit difficult to get installed. This is particularly true for computers running Windows. For these
reasons, we have already applied the deep learning models to our datasets. However, the code to
do this in R is included with the tutorial if you want to adapt these methods to your own data.
It just requires a little bit more setup (and may be difficult if you are not using Linux or
MacOS).

Because the output of the neural network embeddings can be a bit large for some computers, we have
taking a random subset of 1000 movie posters. Let's read the data in here:

```{r, message=FALSE}
posters <- read_csv(file.path("data", "movie-posters-1000.csv"))
```

The first neural network output that we have is the final layer of a ResNet-50 model. The dataset
has 25 rows for each input image, and gives the most probably objects contained in the image. Read
the dataset in with the code below, and make sure that you understand how it is structured:

```{r, message=FALSE}
resnet <- read_csv(file.path("models", "movie-posters-1000-resnet50.csv"))
resnet
```

What types of objects are most common in the data predictions? We can take the most probable object
from each photo and count the number of objects of each type:

```{r}
filter(resnet, rank == 1) %>%
  count(class_description, sort=TRUE)
```

Why do you think there are so many websites, book jackets, televisions, and jigsaw puzzels? 

Let's try to see some of the objects that were detected. Here are all of the images that detected
a military uniform:

```{r}
index <- which(resnet$rank == 1 & resnet$class_description == "military_uniform")
show_image(file.path("images", "movie-posters", resnet$path[index]), ncol=2)
```

Are all of these predictions correct? When they are not, can you figure out how the model was being
mistaken? Here is another example of suits detected in the images:

```{r}
index <- which(resnet$rank == 1 & resnet$class_description == "suit")
show_image(file.path("images", "movie-posters", resnet$path[index]), ncol=4)
```

How well does that model do? Here are bowties:

```{r}
index <- which(resnet$rank == 1 & resnet$class_description == "bow_tie")
show_image(file.path("images", "movie-posters", resnet$path[index]), ncol=3)
```

And seashores:

```{r}
index <- which(resnet$rank == 1 & resnet$class_description == "fountain")
show_image(file.path("images", "movie-posters", resnet$path[index]), ncol=3)
```

How well do these final two object types perform? Where are there mistakes? Can you understand what is
being misunderstood? Feel free to try some other categories (but be careful not to pick categories with
too many objects).

In general, the object detection has trouble for two reasons. First, the images are too small. The model
was trained on higher-resolution images and has trouble with anything other than object types that dominate
the visual space, such as a suit, or objects with very well-defined shapes such as a bow-tie. The second
challenge is that that object model only knows about 1000 types of objects and these do not capture the
diverse set of objects found in the movie posters.


# Embeddings

The object detection algorithm sometimes works well with our posters. When it does, it provides a way of
indexing and searching the collection based on keywords, and perhaps doing an analysis at scale of the
objects found in movie posters. However, for our usage, the pre-build model really is not good enough 
to make practical use of the results beyond a few specific applications. We would need to annotate and
train our own model (outside the scope of the workshop here and very time consuming). There is, however,
another way to make use of the neural networks in our movie poster dataset through image embeddings.

Let's now read in a matrix object giving the penultimate embedding of an image into a high-dimensional 
space.

```{r}
embed <- read_rds(file.path("models", "movie-posters-1000-pool.rds"))
```

The dataset has one row for each image and 2048-columns for each dimension of the embedding:

```{r}
dim(embed)
```

As with word embeddings, the specific columns of the matrix have no explicit meaning. However, images
that are *close* to others share some similar features. We can use this to group images together with
their closest neighbors. We will first build a distance matrix, this has one row and one column for each
images in the dataset.

```{r}
dist_mat <- as.matrix(dist(embed))
dim(dist_mat)
```

We can look at the first ten rows and columns. Notice that each images is a distance 0 away from
itself, and that the matrix is symmetric for example, the distance between image 1 and image 2 is
37.20215; this is the same as the distance between image 2 and image 1).

```{r}
dist_mat[1:10,1:10]
```

We will then compute the closest 15 images to each image in our collection:

```{r}
nearest_img <- t(apply(dist_mat, 2, function(v) order(v)[seq(1, 15)]))
```

Notice that the closest image to each poster is itself (the first column):

```{r}
nearest_img[1:10,]
```

The code below, then, shows the closest images to any starting image that you are interested
in. Start with image 500, as below (note that the starting image is always the one in the upper-left):


```{r}
start_img <- 863
show_image(file.path("images", "movie-posters", posters$path[nearest_img[start_img,]]), ncol=5)
```

Then, try out different starting images. Can you (usually) figure out what features the algorithm is
using to determine that two images are similar? 

How might these similarity scores be used? One example is to build a recommender system that allows
users looking at one image to find other images with common features. 

# Network analysis of image similarity

As our final usage of neural networks on the poster dataset, let's do a large-scale analysis of
the image embeddings. This is interesting because image embeddings are most commonly (in DH at
least) use as a visualization and recommendation technique rather than for analysis of patterns.
To do this, we are going to build a graph (also known as a network), where each node is an image
and images are connected to its three closest images:

```{r}
edges <- rbind(nearest_img[,c(1,2)], nearest_img[,c(1,3)], nearest_img[,c(1,4)])
edges <- unique(t(apply(edges, 1, sort)))

gr <- graph_from_edgelist(edges, directed=FALSE)
gr
```

Now, we can use the graph object to determine the most central images in movie posters:

```{r}
posters$centrality <- eigen_centrality(gr)$vector

posters %>%
  arrange(desc(centrality)) %>%
  select(centrality, title)
```

Any idea why these are the most central images?

```{r}
top_central <- posters %>%
  top_n(centrality, n=15)


index <- which(posters$title %in% top_central$title)
show_image(file.path("images", "movie-posters", posters$path[index]), ncol=5)
```

We can also look at clusters of posters:

```{r}
set.seed(1) # make sure the results are always the same
posters$cluster <- as.integer(membership(cluster_label_prop(gr)))
lapply(split(posters$title, posters$cluster), function(v) v[seq(1, min(length(v), 16))])
```

Notice any interesting patterns? What if we look at some of the clusters:

```{r}
cluster <- 30
index <- which(posters$cluster == cluster)
if (length(index) > 15) index <- sample(index, 15)

show_image(file.path("images", "movie-posters", posters$path[index]), ncol=5)
```

Try some other clusters and try to see if there are any interesting patterns.
